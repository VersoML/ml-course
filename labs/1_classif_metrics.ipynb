{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d36e72b2",
   "metadata": {},
   "source": [
    "# Lab: Binary Classification & Metrics from Scratch\n",
    " \n",
    "## Objectives\n",
    "1.  Train a simple **Logistic Regression** model using `scikit-learn`.\n",
    "2.  Understand the difference between `.predict()` and `.predict_proba()`.\n",
    "3.  **Implement evaluation metrics from scratch** (without using `sklearn.metrics` functions) to understand the mathematics behind them.\n",
    "4.  Visualize the Confusion Matrix, ROC Curve, and Precision-Recall Curve.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cf4c92",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading \n",
    "We will use the **Breast Cancer Wisconsin dataset**, a classic binary classification dataset.\n",
    "* **Input (X):** Features computed from a digitized image of a fine needle aspirate (FNA) of a breast mass.\n",
    "* **Target (y):** Diagnosis (0 = Malignant, 1 = Benign).\n",
    "\n",
    "*Note: We are using standard libraries for data handling and plotting, but we will avoid `sklearn.metrics`.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b813947",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba5a5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "print(f\"Feature shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y)} (0: Malignant, 1: Benign)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a2d1a0",
   "metadata": {},
   "source": [
    "## 2. Data Splitting and Training\n",
    "\n",
    "To evaluate a model properly, we must train it on one set of data and test it on unseen data.\n",
    "\n",
    "**Task:**\n",
    "1.  Import `train_test_split` from `sklearn.model_selection`.\n",
    "2.  Split `X` and `y` into training and testing sets. Use `test_size=0.2` and `random_state=42`.\n",
    "3.  Initialize a `LogisticRegression` model (use `max_iter=10000` to ensure convergence).\n",
    "4.  Fit the model on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec3c833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 2. Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Initialize model\n",
    "model = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# 4. Fit model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b551e6a",
   "metadata": {},
   "source": [
    "## 3. Predictions\n",
    " \n",
    "Scikit-learn models usually provide two methods for prediction:\n",
    "* `.predict(X)`: Returns the hard class labels (0 or 1).\n",
    "* `.predict_proba(X)`: Returns the probability estimates for each class.\n",
    " \n",
    "**Task:** Generate predictions for the **Test Set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93553efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hard predictions (0 or 1)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Get probabilities\n",
    "# Note: predict_proba returns [prob_class_0, prob_class_1]. We usually want prob_class_1.\n",
    "y_prob = model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f32e309",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"First 5 predictions: {y_pred[:5]}\")\n",
    "print(f\"First 5 probabilities (of class 1): {y_prob[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6892177d",
   "metadata": {},
   "source": [
    "## 4. The Confusion Matrix (From Scratch)\n",
    "\n",
    "A confusion matrix summarizes the performance of a classification algorithm.\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "TN & FP \\\\\n",
    " FN & TP\n",
    " \\end{bmatrix}\n",
    " $$\n",
    " \n",
    " * **True Positive (TP):** Model predicted 1, Actual was 1.\n",
    " * **True Negative (TN):** Model predicted 0, Actual was 0.\n",
    " * **False Positive (FP):** Model predicted 1, Actual was 0 (Type I Error).\n",
    " * **False Negative (FN):** Model predicted 0, Actual was 1 (Type II Error).\n",
    " \n",
    "**Task:** Calculate TP, TN, FP, and FN using `y_test` and `y_pred`. Do **not** use `sklearn.metrics.confusion_matrix`.\n",
    " \n",
    "*Hint: You can use boolean masking or numpy summation. E.g., `((y_test == 1) & (y_pred == 1)).sum()`*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ebc11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac2be53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"TP: {TP}, TN: {TN}, FP: {FP}, FN: {FN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555c3c29",
   "metadata": {},
   "source": [
    "### Visualization: Confusion Matrix\n",
    "Run the cell below to visualize your matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b41b63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_custom_confusion_matrix(tp, tn, fp, fn):\n",
    "    matrix = np.array([[tn, fp], [fn, tp]])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                xticklabels=['Predicted 0', 'Predicted 1'],\n",
    "                yticklabels=['Actual 0', 'Actual 1'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "plot_custom_confusion_matrix(TP, TN, FP, FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed483761",
   "metadata": {},
   "source": [
    "## 5. Classification Metrics (From Scratch)\n",
    " \n",
    "Now we will calculate the core metrics using the variables (TP, TN, FP, FN) you calculated above.\n",
    " \n",
    "### Definitions\n",
    " \n",
    " 1.  **Accuracy:** How often is the classifier correct overall?\n",
    "     $$ Accuracy = \\frac{TP + TN}{TP + TN + FP + FN} $$\n",
    " \n",
    " 2.  **Recall (Sensitivity):** Out of all actual positives, how many did we identify?\n",
    "     $$ Recall = \\frac{TP}{TP + FN} $$\n",
    " \n",
    " 3.  **Precision:** Out of all predicted positives, how many were actually positive?\n",
    "     $$ Precision = \\frac{TP}{TP + FP} $$\n",
    " \n",
    " 4.  **F1 Score:** The harmonic mean of Precision and Recall.\n",
    "     $$ F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall} $$\n",
    " \n",
    " **Task:** Implement these formulas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1bf7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 0\n",
    "recall = 0\n",
    "precision = 0\n",
    "f1_score = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a89c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12853fbe",
   "metadata": {},
   "source": [
    "## 6. Thresholds and Curves\n",
    " \n",
    "The predictions (`y_pred`) you generated earlier used a default threshold of **0.5**.\n",
    "If $Probability > 0.5$, predict 1. Otherwise, predict 0.\n",
    " \n",
    "However, we can change this threshold to trade off Precision for Recall.\n",
    " \n",
    "### Task: Threshold Helper Function\n",
    "Write a function that takes the probabilities (`y_prob`), the actual labels (`y_test`), and a specific `threshold`. It should return the **True Positive Rate (TPR)** and **False Positive Rate (FPR)** for that specific threshold.\n",
    " \n",
    "$$ TPR (Recall) = \\frac{TP}{TP + FN} $$\n",
    "$$ FPR = \\frac{FP}{FP + TN} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecad3686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tpr_fpr(y_true, y_probs, threshold):\n",
    "    \n",
    "    # 1. Create temporary predictions based on threshold\n",
    "    # temp_pred = (y_probs >= threshold).astype(int)\n",
    "    \n",
    "    # 2. Calculate TP, FN, FP, TN manually\n",
    "    # tp = ...\n",
    "    # fn = ...\n",
    "    # fp = ...\n",
    "    # tn = ...\n",
    "    \n",
    "    # 3. Calculate Rates\n",
    "    # tpr = ...\n",
    "    # fpr = ...\n",
    "    \n",
    "    return 0.0, 0.0 # return tpr, fpr\n",
    "\n",
    "# Test your function with threshold 0.5 (Should match your previous Recall)\n",
    "# t, f = calculate_tpr_fpr(y_test, y_prob, 0.5)\n",
    "# print(f\"Threshold 0.5 -> TPR: {t}, FPR: {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906c12ea",
   "metadata": {},
   "source": [
    "### Visualization: ROC and PR Curves\n",
    "\n",
    "The following code uses your `calculate_tpr_fpr` function to generate the curves. You do not need to write code here, just run it to verify your logic.\n",
    "\n",
    "1.  **ROC Curve (Receiver Operating Characteristic):** Plots TPR vs FPR. Ideally, it hugs the top-left corner.\n",
    "2.  **Precision-Recall Curve:** Plots Precision vs Recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef64c791",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0, 1, 101)\n",
    "\n",
    "tprs = []\n",
    "fprs = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "# Loop through thresholds and calculate metrics using YOUR function\n",
    "for thresh in thresholds:\n",
    "    # We use a try/except block to handle division by zero in precision\n",
    "    try:\n",
    "        tpr, fpr = calculate_tpr_fpr(y_test, y_prob, thresh)\n",
    "        \n",
    "        # We need to recalculate precision for the PR curve\n",
    "        # Re-using logic for simplicity here, though ideally would be in the function\n",
    "        temp_pred = (y_prob >= thresh).astype(int)\n",
    "        tp = ((y_test == 1) & (temp_pred == 1)).sum()\n",
    "        fp = ((y_test == 0) & (temp_pred == 1)).sum()\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 1.0\n",
    "        \n",
    "        tprs.append(tpr)\n",
    "        fprs.append(fpr)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(tpr) # Recall is same as TPR\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dfe1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# ROC Curve\n",
    "axes[0].plot(fprs, tprs, color='darkorange', lw=2, label='ROC curve')\n",
    "axes[0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "axes[0].set_xlim([0.0, 1.0])\n",
    "axes[0].set_ylim([0.0, 1.05])\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('Receiver Operating Characteristic (ROC)')\n",
    "axes[0].legend(loc=\"lower right\")\n",
    "\n",
    "# Precision-Recall Curve\n",
    "axes[1].plot(recalls, precisions, color='blue', lw=2, label='PR curve')\n",
    "axes[1].set_xlim([0.0, 1.0])\n",
    "axes[1].set_ylim([0.0, 1.05])\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Precision-Recall Curve')\n",
    "axes[1].legend(loc=\"lower left\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
